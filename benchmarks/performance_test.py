"""
Performance benchmarking script for TaskForge
æµ‹è¯•ä¼˜åŒ–å‰åçš„æ€§èƒ½å¯¹æ¯”
"""

import asyncio
import time
from datetime import datetime, timezone, timedelta
from pathlib import Path
import shutil
from typing import List

from taskforge.core.task import Task, TaskStatus, TaskPriority
from taskforge.core.queries import TaskQuery
from taskforge.storage.json_storage import JSONStorage
from taskforge.utils.performance import get_metrics, clear_metrics


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    def __init__(self, data_dir: str = "./benchmark_data"):
        self.data_dir = Path(data_dir)
        self.storage = None

    async def setup(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        # æ¸…ç†æ—§æ•°æ®
        if self.data_dir.exists():
            shutil.rmtree(self.data_dir)

        # åˆå§‹åŒ–å­˜å‚¨
        self.storage = JSONStorage(str(self.data_dir))
        await self.storage.initialize()
        clear_metrics()

    async def teardown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if self.storage:
            await self.storage.cleanup()
        if self.data_dir.exists():
            shutil.rmtree(self.data_dir)

    def create_sample_tasks(self, count: int) -> List[Task]:
        """åˆ›å»ºæ ·æœ¬ä»»åŠ¡"""
        tasks = []
        statuses = list(TaskStatus)
        priorities = list(TaskPriority)

        for i in range(count):
            task = Task(
                title=f"Task {i}",
                description=f"Description for task {i}",
                status=statuses[i % len(statuses)],
                priority=priorities[i % len(priorities)],
                project_id=f"project-{i % 10}",
                assigned_to=f"user-{i % 5}",
                due_date=datetime.now(timezone.utc) + timedelta(days=i % 30)
            )
            tasks.append(task)

        return tasks

    async def benchmark_bulk_create(self, count: int = 1000):
        """æµ‹è¯•æ‰¹é‡åˆ›å»ºæ€§èƒ½"""
        print(f"\nğŸ“ æ‰¹é‡åˆ›å»º {count} ä¸ªä»»åŠ¡...")

        tasks = self.create_sample_tasks(count)

        start_time = time.perf_counter()
        await self.storage.bulk_create_tasks(tasks)
        end_time = time.perf_counter()

        duration = end_time - start_time
        rate = count / duration

        print(f"âœ… å®Œæˆ: {duration:.3f}ç§’")
        print(f"âš¡ é€Ÿç‡: {rate:.1f} tasks/sec")

        return duration, rate

    async def benchmark_search_by_status(self):
        """æµ‹è¯•æŒ‰çŠ¶æ€æŸ¥è¯¢æ€§èƒ½"""
        print(f"\nğŸ” æŒ‰çŠ¶æ€æŸ¥è¯¢...")

        query = TaskQuery(status=[TaskStatus.TODO, TaskStatus.IN_PROGRESS])

        start_time = time.perf_counter()
        results = await self.storage.search_tasks(query, "user-1")
        end_time = time.perf_counter()

        duration = end_time - start_time

        print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªä»»åŠ¡")
        print(f"âš¡ ç”¨æ—¶: {duration*1000:.2f}ms")

        return duration, len(results)

    async def benchmark_search_by_project(self):
        """æµ‹è¯•æŒ‰é¡¹ç›®æŸ¥è¯¢æ€§èƒ½"""
        print(f"\nğŸ“ æŒ‰é¡¹ç›®æŸ¥è¯¢...")

        query = TaskQuery(project_id="project-5")

        start_time = time.perf_counter()
        results = await self.storage.search_tasks(query, "user-1")
        end_time = time.perf_counter()

        duration = end_time - start_time

        print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªä»»åŠ¡")
        print(f"âš¡ ç”¨æ—¶: {duration*1000:.2f}ms")

        return duration, len(results)

    async def benchmark_complex_query(self):
        """æµ‹è¯•å¤åˆæŸ¥è¯¢æ€§èƒ½"""
        print(f"\nğŸ” å¤åˆæŸ¥è¯¢ (çŠ¶æ€+ä¼˜å…ˆçº§+é¡¹ç›®)...")

        query = TaskQuery(
            status=[TaskStatus.TODO],
            priority=[TaskPriority.HIGH, TaskPriority.CRITICAL],
            project_id="project-3"
        )

        start_time = time.perf_counter()
        results = await self.storage.search_tasks(query, "user-1")
        end_time = time.perf_counter()

        duration = end_time - start_time

        print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªä»»åŠ¡")
        print(f"âš¡ ç”¨æ—¶: {duration*1000:.2f}ms")

        return duration, len(results)

    async def benchmark_bulk_update(self, count: int = 500):
        """æµ‹è¯•æ‰¹é‡æ›´æ–°æ€§èƒ½"""
        print(f"\nâœï¸ æ‰¹é‡æ›´æ–° {count} ä¸ªä»»åŠ¡...")

        # è·å–è¦æ›´æ–°çš„ä»»åŠ¡
        query = TaskQuery(limit=count)
        tasks = await self.storage.search_tasks(query, "user-1")

        # æ›´æ–°çŠ¶æ€
        for task in tasks:
            task.status = TaskStatus.DONE

        start_time = time.perf_counter()
        await self.storage.bulk_update_tasks(tasks)
        end_time = time.perf_counter()

        duration = end_time - start_time
        rate = count / duration

        print(f"âœ… å®Œæˆ: {duration:.3f}ç§’")
        print(f"âš¡ é€Ÿç‡: {rate:.1f} updates/sec")

        return duration, rate

    async def benchmark_statistics(self):
        """æµ‹è¯•ç»Ÿè®¡æŸ¥è¯¢æ€§èƒ½"""
        print(f"\nğŸ“Š ç»Ÿè®¡æŸ¥è¯¢...")

        start_time = time.perf_counter()
        stats = await self.storage.get_task_statistics()
        end_time = time.perf_counter()

        duration = end_time - start_time

        print(f"âœ… ç»Ÿè®¡å®Œæˆ")
        print(f"   - æ€»ä»»åŠ¡æ•°: {stats['total_tasks']}")
        print(f"   - å®Œæˆç‡: {stats['completion_rate']*100:.1f}%")
        print(f"âš¡ ç”¨æ—¶: {duration*1000:.2f}ms")

        return duration, stats

    async def run_full_benchmark(self):
        """è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•"""
        print("=" * 60)
        print("ğŸš€ TaskForge æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 60)

        try:
            await self.setup()

            # 1. æ‰¹é‡åˆ›å»ºæµ‹è¯•
            create_time, create_rate = await self.benchmark_bulk_create(1000)

            # 2. æŸ¥è¯¢æµ‹è¯•
            status_time, status_count = await self.benchmark_search_by_status()
            project_time, project_count = await self.benchmark_search_by_project()
            complex_time, complex_count = await self.benchmark_complex_query()

            # 3. æ‰¹é‡æ›´æ–°æµ‹è¯•
            update_time, update_rate = await self.benchmark_bulk_update(500)

            # 4. ç»Ÿè®¡æµ‹è¯•
            stats_time, stats = await self.benchmark_statistics()

            # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡æ‘˜è¦
            print("\n" + "=" * 60)
            print("ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡æ‘˜è¦")
            print("=" * 60)

            metrics = get_metrics()
            if metrics:
                print("\nğŸ”¥ çƒ­ç‚¹å‡½æ•°:")
                sorted_metrics = sorted(
                    metrics.items(),
                    key=lambda x: x[1]['avg'],
                    reverse=True
                )[:5]

                for name, data in sorted_metrics:
                    print(f"   {name}:")
                    print(f"      å¹³å‡: {data['avg']*1000:.2f}ms")
                    print(f"      è°ƒç”¨: {data['count']}æ¬¡")

            print("\nâœ… åŸºå‡†æµ‹è¯•å®Œæˆ!")

        finally:
            await self.teardown()


async def main():
    """ä¸»å‡½æ•°"""
    benchmark = PerformanceBenchmark()
    await benchmark.run_full_benchmark()


if __name__ == "__main__":
    asyncio.run(main())
